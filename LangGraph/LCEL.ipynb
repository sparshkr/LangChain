{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetch environment variables with a fallback to avoid None errors\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "LANGSMITH_PROJECT = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "LANGSMITH_ENDPOINT = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "LANGSMITH_TRACING = os.getenv(\"LANGSMITH_TRACING\")\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"TAVILY_API_KEY\"] = TAVILY_API_KEY\n",
    "os.environ[\"GROQ_API_KEY\"] = GROQ_API_KEY\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = LANGSMITH_API_KEY\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = LANGSMITH_PROJECT\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = LANGSMITH_ENDPOINT\n",
    "os.environ[\"LANGSMITH_TRACING\"] = LANGSMITH_TRACING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\Desktop\\LangGraph\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template= 'Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n",
    "\n",
    "prompt = PromptTemplate(template=template,input_variables=[\"skill\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['skill'], input_types={}, partial_variables={}, template='Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To effectively learn Big Data, focus on these 5 key areas:\n",
      "\n",
      "1. **Programming Languages (Python or Java):**  Big Data processing relies heavily on programming. Python is preferred for its ease of use, extensive libraries (like Pandas, NumPy, Scikit-learn for data manipulation and analysis), and strong community support. Java is also crucial, especially for working with Hadoop and Spark ecosystems.  Choose one to start with and become proficient; you can always add the other later.\n",
      "\n",
      "2. **Hadoop Ecosystem:**  Understanding the fundamentals of Hadoop (HDFS for storage and MapReduce for processing) is essential. While newer tools like Spark are gaining popularity, Hadoop remains a cornerstone of many Big Data architectures.  Focus on concepts like data partitioning, data replication, and the MapReduce paradigm.  Don't get bogged down in the minutiae of Hadoop's underlying implementation unless you plan on becoming a Hadoop administrator.\n",
      "\n",
      "3. **Apache Spark:** Spark is a fast, in-memory data processing engine that significantly outperforms Hadoop MapReduce for many tasks. Learn its core concepts: Resilient Distributed Datasets (RDDs), transformations, actions, and its various libraries (e.g., Spark SQL, Spark Streaming, MLlib for machine learning).  This is arguably *the* most important technology to master in the Big Data landscape today.\n",
      "\n",
      "4. **Databases (SQL and NoSQL):**  Big Data involves managing massive datasets.  You'll need to understand both relational databases (SQL) and NoSQL databases (like MongoDB, Cassandra, or HBase).  SQL is crucial for structured data, while NoSQL is better suited for unstructured or semi-structured data.  Learn the strengths and weaknesses of each type and when to use them.\n",
      "\n",
      "5. **Data Warehousing and ETL (Extract, Transform, Load):**  Big Data isn't just about processing; it's also about organizing and making sense of the data.  Learn the principles of data warehousing, including designing schemas, choosing the right database technology, and the ETL process of extracting data from various sources, transforming it into a consistent format, and loading it into the warehouse for analysis and reporting.  Tools like Apache Kafka are often used in this process.\n",
      "\n",
      "\n",
      "These five areas provide a solid foundation.  Once you're comfortable with these, you can explore more specialized topics like cloud-based Big Data services (AWS EMR, Azure HDInsight, Google Cloud Dataproc), data visualization tools (Tableau, Power BI), and specific machine learning algorithms. Remember to practice consistently with real-world projects to solidify your understanding.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({'skill':'Big Data'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough , RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_upper(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnablePassthrough() | RunnableLambda(string_upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABCD'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"abcd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel({'x':RunnablePassthrough(),'y':RunnablePassthrough()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 'Hello', 'y': 'Hello'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'Youtube': 'abcd', 'Blog': 'abc blog'},\n",
       " 'y': {'Youtube': 'abcd', 'Blog': 'abc blog'}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'Youtube': 'abcd','Blog': \"abc blog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abc blog'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = lambda x: x['Blog'] ## lambda is a anonymous function in Python\n",
    "func({'Youtube': 'abcd','Blog': \"abc blog\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_website(input: dict):\n",
    "    output = input.get('Website','Not found')\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 'Not found', 'Blog': 'abc blog'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableParallel({'x':RunnablePassthrough() | RunnableLambda(fetch_website),'Blog':lambda x: x['Blog']})\n",
    "chain.invoke({'Youtube': 'abcd','Blog': \"abc blog\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "### Reading the txt files from source directory\n",
    "\n",
    "# loader = DirectoryLoader('./source', glob=\"./*.txt\", loader_cls=TextLoader)\n",
    "# docs = loader.load()\n",
    "loader = TextLoader('./source.txt')\n",
    "docs = loader.load()\n",
    "\n",
    "### Creating Chunks using RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "new_docs = text_splitter.split_documents(documents=docs)\n",
    "doc_strings = [doc.page_content for doc in new_docs]\n",
    "\n",
    "###  BGE Embddings\n",
    "\n",
    "'''from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs,\n",
    ")\n",
    "'''\n",
    "\n",
    "### Creating Retriever using Vector DB\n",
    "\n",
    "db = Chroma.from_documents(new_docs, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =\"What is my name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsh\n"
     ]
    }
   ],
   "source": [
    "result = await retrieval_chain.ainvoke(question)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'Hi! I am learning {skill}. Can you suggest me top 5 things to learn?\\n'\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For someone starting to learn Data Structures and Algorithms (DSA), focusing on these 5 foundational areas will provide the strongest base:\n",
      "\n",
      "1. **Big O Notation:** This is crucial.  Understanding Big O notation allows you to analyze the efficiency of your algorithms in terms of time and space complexity.  Don't just memorize the notations; understand *why* an algorithm has a particular time complexity (e.g., O(n), O(log n), O(n^2)). This will inform your choices when designing algorithms.\n",
      "\n",
      "2. **Arrays and Linked Lists:** These are fundamental data structures.  Mastering their implementations (including variations like doubly linked lists, circular linked lists) and understanding their strengths and weaknesses (e.g., array access vs. linked list insertion/deletion) is essential.\n",
      "\n",
      "3. **Searching and Sorting Algorithms:**  Learn at least one algorithm from each category:\n",
      "    * **Searching:** Linear search and Binary search (understand when each is appropriate).\n",
      "    * **Sorting:**  Bubble sort (for understanding the basics), Merge sort (efficient and often used in real-world applications), and Quick sort (very efficient but can be tricky to implement correctly).  Understanding the time and space complexities of each is key.\n",
      "\n",
      "4. **Basic Tree Structures:** Start with binary trees and binary search trees (BSTs).  Understand tree traversals (inorder, preorder, postorder) and how to perform operations like insertion, deletion, and search within a BST.  This will lay the groundwork for more advanced tree structures (like heaps, tries, etc.) you'll encounter later.\n",
      "\n",
      "5. **Hash Tables (Hash Maps):**  Hash tables are incredibly important for implementing dictionaries and sets.  Understand how they work (hash functions, collision handling), their average-case and worst-case time complexities, and their applications.\n",
      "\n",
      "\n",
      "**Important Note:**  Learning DSA is about more than just memorizing algorithms.  Focus on understanding the underlying principles, the trade-offs between different approaches, and how to choose the right data structure and algorithm for a specific problem.  Practice, practice, practice by solving problems on platforms like LeetCode, HackerRank, or Codewars."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({'skill':'Data Structure and Algorithms'}):\n",
    "    print(s.content,end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
